development:
  ollama:
    service: "Ollama"
    api_key: ""
    model: "ai/gemma3"
    host: "http://model-runner.docker.internal:12434/engines/v1/"
    temperature: 0.7
